{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "645eae97",
   "metadata": {},
   "source": [
    "## Using VLLM embedings for retrieval tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7430b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "base_dir = \"/home/xiuyan.wu/beir\"\n",
    "sys.path.append(base_dir)\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dded6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-09 11:31:22 [utils.py:261] non-default args: {'tensor_parallel_size': 2, 'disable_log_stats': True, 'model': 'Qwen/Qwen3-30B-A3B-Instruct-2507-FP8'}\n",
      "INFO 02-09 11:31:23 [model.py:541] Resolved architecture: Qwen3MoeForCausalLM\n",
      "INFO 02-09 11:31:23 [model.py:1561] Using max model len 262144\n",
      "INFO 02-09 11:31:25 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 02-09 11:31:25 [vllm.py:624] Asynchronous scheduling is enabled.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m INFO 02-09 11:31:26 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='Qwen/Qwen3-30B-A3B-Instruct-2507-FP8', speculative_config=None, tokenizer='Qwen/Qwen3-30B-A3B-Instruct-2507-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=262144, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-30B-A3B-Instruct-2507-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['+quant_fp8', 'none', '+quant_fp8'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m WARNING 02-09 11:31:26 [multiproc_executor.py:910] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m INFO 02-09 11:31:26 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:39829 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m INFO 02-09 11:31:26 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:39829 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m INFO 02-09 11:31:27 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "[Gloo] Rank [Gloo] Rank 01 is connected to  is connected to 11 peer ranks.  peer ranks. Expected number of connected peer ranks is : Expected number of connected peer ranks is : 11\n",
      "\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m \u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m WARNING 02-09 11:31:27 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.\n",
      "WARNING 02-09 11:31:27 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m \u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m WARNING 02-09 11:31:27 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 02-09 11:31:27 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m INFO 02-09 11:31:27 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m INFO 02-09 11:31:27 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=2550663)\u001b[0;0m INFO 02-09 11:31:28 [gpu_model_runner.py:4033] Starting to load model Qwen/Qwen3-30B-A3B-Instruct-2507-FP8...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=2550663)\u001b[0;0m INFO 02-09 11:31:29 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=2550663)\u001b[0;0m INFO 02-09 11:31:29 [fp8.py:328] Using MARLIN Fp8 MoE backend out of potential backends: ['AITER', 'FLASHINFER_TRTLLM', 'FLASHINFER_CUTLASS', 'DEEPGEMM', 'BATCHED_DEEPGEMM', 'TRITON', 'BATCHED_TRITON', 'MARLIN'].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809c60ae12f34ad699086535ad9d2111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82d4113e3b84e009c9d011bc9d4408b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9f4f73415345a79bdea041f1c10c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a49558a598f480499d70db7c8f591c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b68ed1b88194427ade4c9b6ca6f2bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m \u001b[0;36m(Worker_TP1 pid=2550666)\u001b[0;0m INFO 02-09 11:37:54 [weight_utils.py:527] Time spent downloading weights for Qwen/Qwen3-30B-A3B-Instruct-2507-FP8: 384.554873 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4541c13d395b4a599bed209917fdc4d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m \u001b[0;36m(Worker_TP1 pid=2550666)\u001b[0;0m WARNING 02-09 11:38:03 [marlin_utils_fp8.py:97] Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=2550663)\u001b[0;0m INFO 02-09 11:38:03 [default_loader.py:291] Loading weights took 8.42 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=2550663)\u001b[0;0m WARNING 02-09 11:38:03 [marlin_utils_fp8.py:97] Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=2550663)\u001b[0;0m INFO 02-09 11:38:05 [gpu_model_runner.py:4130] Model loading took 14.86 GiB memory and 396.088065 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=2550663)\u001b[0;0m INFO 02-09 11:38:18 [backends.py:812] Using cache directory: /home/xiuyan.wu/.cache/vllm/torch_compile_cache/2b108e0cf2/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=2550663)\u001b[0;0m INFO 02-09 11:38:18 [backends.py:872] Dynamo bytecode transform time: 13.22 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m \u001b[0;36m(Worker_TP1 pid=2550666)\u001b[0;0m INFO 02-09 11:38:36 [backends.py:302] Cache the graph of compile range (1, 8192) for later use\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=2550663)\u001b[0;0m INFO 02-09 11:38:36 [backends.py:302] Cache the graph of compile range (1, 8192) for later use\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=2550663)\u001b[0;0m INFO 02-09 11:38:50 [backends.py:319] Compiling a graph for compile range (1, 8192) takes 21.85 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=2550663)\u001b[0;0m INFO 02-09 11:38:50 [monitor.py:34] torch.compile takes 35.07 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=2550663)\u001b[0;0m INFO 02-09 11:38:51 [gpu_worker.py:356] Available KV cache memory: 4.38 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ERROR 02-09 11:38:51 [core.py:946] EngineCore failed to start.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ERROR 02-09 11:38:51 [core.py:946] Traceback (most recent call last):\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ERROR 02-09 11:38:51 [core.py:946]   File \"/home/xiuyan.wu/beir/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 937, in run_engine_core\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ERROR 02-09 11:38:51 [core.py:946]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ERROR 02-09 11:38:51 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ERROR 02-09 11:38:51 [core.py:946]   File \"/home/xiuyan.wu/beir/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 691, in __init__\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ERROR 02-09 11:38:51 [core.py:946]     super().__init__(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ERROR 02-09 11:38:51 [core.py:946]   File \"/home/xiuyan.wu/beir/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 112, in __init__\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ERROR 02-09 11:38:51 [core.py:946]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ERROR 02-09 11:38:51 [core.py:946]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ERROR 02-09 11:38:51 [core.py:946]   File \"/home/xiuyan.wu/beir/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 253, in _initialize_kv_caches\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ERROR 02-09 11:38:51 [core.py:946]     kv_cache_configs = get_kv_cache_configs(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ERROR 02-09 11:38:51 [core.py:946]                        ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ERROR 02-09 11:38:51 [core.py:946]   File \"/home/xiuyan.wu/beir/venv/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1516, in get_kv_cache_configs\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ERROR 02-09 11:38:51 [core.py:946]     _check_enough_kv_cache_memory(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ERROR 02-09 11:38:51 [core.py:946]   File \"/home/xiuyan.wu/beir/venv/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py\", line 634, in _check_enough_kv_cache_memory\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ERROR 02-09 11:38:51 [core.py:946]     raise ValueError(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ERROR 02-09 11:38:51 [core.py:946] ValueError: To serve at least one request with the models's max seq len (262144), (12.0 GiB KV cache is needed, which is larger than the available KV cache memory (4.38 GiB). Based on the available memory, the estimated maximum model length is 95632. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m \u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m \u001b[0;36m(Worker_TP0 pid=2550663)\u001b[0;0m \u001b[0;36m(Worker_TP1 pid=2550666)\u001b[0;0m WARNING 02-09 11:38:51 [multiproc_executor.py:786] WorkerProc was terminated\n",
      "WARNING 02-09 11:38:51 [multiproc_executor.py:786] WorkerProc was terminated\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ERROR 02-09 11:38:52 [multiproc_executor.py:246] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m Process EngineCore_DP0:\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m   File \"/home/xiuyan.wu/.pyenv/versions/3.12.12/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m     self.run()\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m   File \"/home/xiuyan.wu/.pyenv/versions/3.12.12/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m   File \"/home/xiuyan.wu/beir/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 950, in run_engine_core\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m     raise e\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m   File \"/home/xiuyan.wu/beir/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 937, in run_engine_core\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m   File \"/home/xiuyan.wu/beir/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 691, in __init__\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m     super().__init__(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m   File \"/home/xiuyan.wu/beir/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 112, in __init__\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m   File \"/home/xiuyan.wu/beir/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 253, in _initialize_kv_caches\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m     kv_cache_configs = get_kv_cache_configs(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m   File \"/home/xiuyan.wu/beir/venv/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py\", line 1516, in get_kv_cache_configs\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m     _check_enough_kv_cache_memory(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m   File \"/home/xiuyan.wu/beir/venv/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py\", line 634, in _check_enough_kv_cache_memory\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m     raise ValueError(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=2550649)\u001b[0;0m ValueError: To serve at least one request with the models's max seq len (262144), (12.0 GiB KV cache is needed, which is larger than the available KV cache memory (4.38 GiB). Based on the available memory, the estimated maximum model length is 95632. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.\n",
      "/home/xiuyan.wu/.pyenv/versions/3.12.12/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mvllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQwen/Qwen3-30B-A3B-Instruct-2507-FP8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/beir/venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:334\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, enable_return_routed_experts, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, structured_outputs_config, profiler_config, attention_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[39m\n\u001b[32m    297\u001b[39m engine_args = EngineArgs(\n\u001b[32m    298\u001b[39m     model=model,\n\u001b[32m    299\u001b[39m     runner=runner,\n\u001b[32m   (...)\u001b[39m\u001b[32m    329\u001b[39m     **kwargs,\n\u001b[32m    330\u001b[39m )\n\u001b[32m    332\u001b[39m log_non_default_args(engine_args)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    339\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/beir/venv/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:172\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[39m\n\u001b[32m    169\u001b[39m     enable_multiprocessing = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[38;5;66;03m# Create the LLMEngine.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/beir/venv/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:106\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, aggregate_engine_logging, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28mself\u001b[39m.output_processor.tracer = tracer\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[38;5;28mself\u001b[39m.logger_manager: StatLoggerManager | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_stats:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/beir/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:94\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient.make_async_mp_client(\n\u001b[32m     90\u001b[39m         vllm_config, executor_class, log_stats\n\u001b[32m     91\u001b[39m     )\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/beir/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:647\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    644\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    645\u001b[39m     \u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor], log_stats: \u001b[38;5;28mbool\u001b[39m\n\u001b[32m    646\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    654\u001b[39m     \u001b[38;5;28mself\u001b[39m.is_dp = \u001b[38;5;28mself\u001b[39m.vllm_config.parallel_config.data_parallel_size > \u001b[32m1\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[EngineCoreOutputs | \u001b[38;5;167;01mException\u001b[39;00m]()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/beir/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:479\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[39m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28mself\u001b[39m.stats_update_address = client_addresses.get(\u001b[33m\"\u001b[39m\u001b[33mstats_update_address\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    478\u001b[39m     \u001b[38;5;66;03m# Engines are managed by this client.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlaunch_core_engines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_manager\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/contextlib.py:144\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/beir/venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py:933\u001b[39m, in \u001b[36mlaunch_core_engines\u001b[39m\u001b[34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m local_engine_manager, coordinator, addresses\n\u001b[32m    932\u001b[39m \u001b[38;5;66;03m# Now wait for engines to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengines_to_handshake\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdp_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_moe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_engine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/beir/venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py:992\u001b[39m, in \u001b[36mwait_for_engine_startup\u001b[39m\u001b[34m(handshake_socket, addresses, core_engines, parallel_config, coordinated_dp, cache_config, proc_manager, coord_process)\u001b[39m\n\u001b[32m    990\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process.exitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    991\u001b[39m         finished[coord_process.name] = coord_process.exitcode\n\u001b[32m--> \u001b[39m\u001b[32m992\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    993\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    994\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSee root cause above. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    995\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    996\u001b[39m     )\n\u001b[32m    998\u001b[39m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[32m    999\u001b[39m eng_identity, ready_msg_bytes = handshake_socket.recv_multipart()\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_DP0': 1}"
     ]
    }
   ],
   "source": [
    "llm =\n",
    "vllm.LLM(model=\"Qwen/Qwen3-30B-A3B-Instruct-2507-FP8\", tensor_parallel_size=2, kv_cache_dtype=\"fp8\", enable_prefix_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded8a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "lll"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
